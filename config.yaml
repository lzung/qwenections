# Fine-tuning configuration for Qwen Connections model

model:
  name: "Qwen/Qwen2.5-1.5B-Instruct"
  max_length: 256
  trust_remote_code: true

training:
  output_dir: "./checkpoints"
  num_train_epochs: 1
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 2
  learning_rate: 2.0e-5
  warmup_steps: 0
  logging_steps: 5
  save_steps: 1000
  eval_steps: 1000
  save_total_limit: 1
  fp16: false
  gradient_checkpointing: false
  optim: "adamw_torch"
  lr_scheduler_type: "cosine"

lora:
  enabled: true
  r: 8
  lora_alpha: 16
  lora_dropout: 0.05
  target_modules:
    [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj",
    ]

data:
  train_file: "./data/processed/train.jsonl"
  eval_file: "./data/processed/eval.jsonl"
  max_samples: 50

prompts:
  template_file: "prompt_templates.yaml"
  style: "default" # Options: default, concise, detailed, iterative, all_at_once
