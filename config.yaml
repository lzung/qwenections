# Fine-tuning configuration for Qwen Connections model

model:
  name: "Qwen/Qwen2.5-4B-Instruct" # Can be changed to other Qwen variants
  max_length: 2048
  trust_remote_code: true

training:
  output_dir: "./checkpoints"
  num_train_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 2.0e-5
  warmup_steps: 100
  logging_steps: 10
  save_steps: 500
  eval_steps: 500
  save_total_limit: 3
  fp16: true
  gradient_checkpointing: true
  optim: "adamw_torch"
  lr_scheduler_type: "cosine"

lora:
  enabled: true
  r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules:
    [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj",
    ]

data:
  train_file: "./data/processed/train.jsonl"
  eval_file: "./data/processed/eval.jsonl"
  max_samples: null # null means use all data

